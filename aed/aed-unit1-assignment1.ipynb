{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria de Dados em Python\n",
    "\n",
    "Neste notebook, vamos praticar algumas das t√©cnicas e processos de pensamento para an√°lise explorat√≥ria de dados. Vamos praticar a an√°lise com um conjunto de dados real. √â importante enfatizar que n√£o existe um √∫nico caminho correto para a an√°lise explorat√≥ria de dados. A maneira como voc√™ explora seus dados depende das perguntas que voc√™ est√° tentando responder e dos dados em si. Dessa forma, deixe os dados e sua curiosidade gui√°-lo.\n",
    "\n",
    "Se for necess√°rio acrescentar passos adicionais al√©m daqueles que est√£o indicados neste notebook, fa√ßa isso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Conjunto de Dados\n",
    "\n",
    "Este conjunto de dados √© basicamente uma tabela de amostras com dados de abund√¢ncia de prote√≠nas nucleares. Essas amostras s√£o de camundongos normais ou de camundongos com uma condi√ß√£o semelhante √† _S√≠ndrome de Down_. Alguns desses camundongos receberam uma droga chamada _memantina_, que os pesquisadores conjecturam que poderia resgatar a fun√ß√£o cognitiva em camundongos afetados. Existem 38 ratos normais e 34 ratos afetados, e cada rato contribuiu com 15 amostras para o conjunto de dados, para um total de 1080 amostras. A raz√£o de abund√¢ncia de 77 prote√≠nas √© medida para cada amostra.\n",
    "\n",
    "O conjunto de dados pode ser obtido no [aqui](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression). Acesse o link `Data Folder`, e salve o arquivo `Data_Cortex_Nuclear.xls` na mesma pasta em que voc√™ gravou este notebook. Baixe este arquivo, e carregue-o em um dataframe do Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data_Cortex_Nuclear.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75f015b56dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data_Cortex_Nuclear.xls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     return io.parse(\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_Cortex_Nuclear.xls'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Data_Cortex_Nuclear.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida inspecione o dataframe para verificar se h√° dados faltantes. Caso haja, preencha estes dados com a m√©dia dos dados existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df_nulls(df, verbose=False):\n",
    "    \n",
    "    # get columns with float64 values\n",
    "    is_float = []\n",
    "    for i in df.dtypes.keys():\n",
    "        if df.dtypes[i] == 'float64':\n",
    "            is_float.append(i)\n",
    "\n",
    "    # get columns with null values      \n",
    "    has_null = df.isnull().any()\n",
    "    for i in has_null.keys():\n",
    "        if has_null[i]:\n",
    "            print(i, '=>', df[i].isnull().sum(), 'nulls')\\\n",
    "            if verbose else None\n",
    "        else: has_null.pop(i)\n",
    "\n",
    "    # get columns with null float64 values\n",
    "    to_fill = [value for value in is_float if value in has_null.keys()]\n",
    "\n",
    "    # print overview\n",
    "    print(len(df.keys()), 'total columns')\n",
    "    print(len(is_float), 'are float64')\n",
    "    print(len(has_null), 'contain nulls')\n",
    "    print(len(to_fill), 'columns to fill\\n')\n",
    "    # return to_fill\n",
    "\n",
    "# first check\n",
    "print('Checking data frame...')\n",
    "check_df_nulls(df)\n",
    "    \n",
    "# fill values with mean\n",
    "print('Filling null values with mean...', end=' ')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# for i in to_fill:\n",
    "#     df[i] = df[i].fillna(df[i].mean())\n",
    "    \n",
    "# second check\n",
    "print('OK.\\n\\nRechecking...')\n",
    "check_df_nulls(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Estat√≠sticas de Resumo Adicionais\n",
    "\n",
    "Os dataframes do Pandas t√™m um m√©todo pr√°tico chamado `describe` que escreve uma variedade de estat√≠sticas de resumo para cada vari√°vel. As estat√≠sticas de resumo retornadas por esse m√©todo tamb√©m s√£o um dataframe, portanto, voc√™ pode alter√°-lo facilmente para incluir outras estat√≠sticas. Vamos fazer exatamente isso, adicionando o coeficiente de varia√ß√£o da vari√°vel (cova).\n",
    "\n",
    "O coeficiente de varia√ß√£o √© apenas a raz√£o entre o desvio padr√£o e a m√©dia da distribui√ß√£o. Distribui√ß√µes com um grande coeficiente de varia√ß√£o s√£o mais propensas a sinalizar uma mudan√ßa no processo de gera√ß√£o de dados subjacente, ao contr√°rio da varia√ß√£o aleat√≥ria.\n",
    "\n",
    "Adicionar uma nova linha no dataframe de `resumo` pode ser realizado da seguinte forma `resumo.loc['cova'] = ¬´dados da nova linha¬ª`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumo = df.describe()\n",
    "resumo.loc['cova'] = df.std()/df.mean()\n",
    "resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normaliza√ß√£o\n",
    "\n",
    "Um passo inicial importante antes de qualquer an√°lise real √© realizado para normalizar os dados. Isso √© necess√°rio para evitar que vari√°veis com valores grandes tenham uma influ√™ncia indevida nos resultados. Existem alguns m√©todos diferentes de normalizar seus dados:\n",
    "\n",
    "+ O m√©todo mais comum √© dividir os valores de uma dimens√£o pelo maior valor que ela cont√©m. Esse m√©todo √© f√°cil de interpretar e √© robusto para outliers quando o coeficiente de varia√ß√£o em uma dimens√£o √© baixo.\n",
    "+ Outro m√©todo muito comum √© subtrair a m√©dia e dividir a dimens√£o por seu desvio padr√£o, um processo √†s vezes chamado de padroniza√ß√£o. Esse m√©todo fornece mais informa√ß√µes sobre a similaridade relativa de valores quando o coeficiente de varia√ß√£o em uma dimens√£o √© alto. Desempenha mal nas dimens√µes com um baixo coeficiente de varia√ß√£o e grandes outliers.\n",
    "+ Uma op√ß√£o subutilizada √© usar a fun√ß√£o de distribui√ß√£o cumulativa da distribui√ß√£o emp√≠rica da dimens√£o para obter uma classifica√ß√£o de percentil. Esse m√©todo funciona bem na maioria dos casos e √© robusto para outliers. Sua utilidade se degrada um pouco quando o coeficiente de varia√ß√£o de uma dimens√£o fica muito pequeno.\n",
    "\n",
    "O m√©todo de normaliza√ß√£o usado depende dos dados e dos aspectos que voc√™ deseja destacar. Voc√™ pode usar um m√©todo de normaliza√ß√£o diferente para cada dimens√£o de seus dados, embora eu n√£o recomende isso, a menos que seja absolutamente necess√°rio. Em geral, minha prefer√™ncia √© pelo m√©todo de distribui√ß√£o emp√≠rica.\n",
    "\n",
    "O c√≥digo em Python necess√°rio para fazer os 3 tipos de normaliza√ß√£o √© exemplificado abaixo (supondo que o dataframe original se chame `df` e contenha apenas dados num√©ricos):\n",
    "\n",
    "    minmax_normalized_df = pd.DataFrame(MinMaxScaler().fit_transform(df), \n",
    "    columns=df_numeric.columns, index=df_numeric.index)\n",
    "    \n",
    "    standardized_df = pd.DataFrame(StandardScaler().fit_transform(df), \n",
    "    columns=df_numeric.columns, index=df_numeric.index)\n",
    "    \n",
    "    ecdf_normalized_df = df.apply(lambda c: pd.Series(ECDF(c)(c), index=c.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns with float values from data frame\n",
    "df_floats = df.loc[:, df.dtypes == float]\n",
    "\n",
    "# normalize values in data frame (MinMax)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalized_df = pd.DataFrame(MinMaxScaler().fit_transform(df_floats),\n",
    "                             columns=df_floats.columns, index=df.index)\n",
    "\n",
    "# normalize values in data frame (Standard)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# normalized_df = pd.DataFrame(StandardScaler().fit_transform(df_floats),\n",
    "#                              columns=df_floats.columns, index=df.index)\n",
    "\n",
    "# normalize data frame (ECDF)\n",
    "# from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# normalized_df = df_floats.apply(lambda c: pd.Series(ECDF(c)(c), index=c.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando dados\n",
    "\n",
    "Usaremos a estrat√©gia de explorar dados procurando coisas que s√£o fora do comum. Em um conjunto de dados que seja apenas vari√°veis aleat√≥rias gaussianas n√£o correlacionadas, n√£o poderiamos aprender nada com ele, seria literalmente ru√≠do branco. A maior parte do conte√∫do da informa√ß√£o na maioria dos conjuntos de dados √© sua varia√ß√£o da normalidade.\n",
    "\n",
    "A distribui√ß√£o do coeficiente de varia√ß√£o informa sobre a variabilidade relativa desses dados. Altos coeficientes de varia√ß√£o para uma vari√°vel indicam um prov√°vel candidato a efeito experimental.\n",
    "\n",
    "Plote o histograma do coeficiente de varia√ß√£o dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# get coefficient_of_variation\n",
    "series_cova = normalized_df.std()/normalized_df.mean()\n",
    "series_cova.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "plt.hist(series_cova, bins=30, alpha=0.7, rwidth=0.9)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Coeficiente de varia√ß√£o')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.title('Histograma')\n",
    "# plt.xlim(left=0.15, right=0.55) # <-- slice it further\n",
    "plt.show()\n",
    "\n",
    "# plot more histograms\n",
    "# for i in df.keys():\n",
    "#     if df.dtypes[i] == 'float64':\n",
    "#         plt.hist(normalized_df[i], bins='auto', alpha=0.7, rwidth=0.9)\n",
    "#         plt.grid(axis='y', alpha=0.75)\n",
    "#         plt.xlabel(str(i))\n",
    "#         plt.ylabel('Frequency')\n",
    "#         plt.title(i)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do gr√°fico acima √© poss√≠vel estimar um valor de corte para o coeficiente de varia√ß√£o? Um valor abaixo do qual poder√≠amos desprezar a altera√ß√£o das prote√≠nas como sendo um efeito do tratamento? Qual seria este valor?\n",
    "\n",
    "Se voc√™ achar que sim, gere um novo dataframe apenas com as prote√≠nas que interessam segundo este crit√©rio (A).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta**: Considerando o gr√°fico acima, argumenta-se que o valor de corte deve visar as prote√≠nas que obtiveram um coeficiente de varia√ß√£o de aproximadamente >= 0.496 entre si, por raz√£o:\n",
    "- a) da grande quantidade de prote√≠nas que exibiram esse padr√£o (de pouco variar entre si);\n",
    "- b) de que distribui√ß√µes com um grande coeficiente de varia√ß√£o s√£o mais propensas a sinalizar uma mudan√ßa no processo de gera√ß√£o de dados subjacente, ao contr√°rio da varia√ß√£o aleat√≥ria.\n",
    "\n",
    "Nota-se que uma an√°lise explorat√≥ria de cada vari√°vel em mais histogramas no conjunto subsequente pode ser √∫til para o entendimento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df.copy()\n",
    "proteins = []\n",
    "\n",
    "# considering <= 0.4\n",
    "float_cova = 0.4\n",
    "for x in series_cova.items():\n",
    "    if x[0] in df_A.keys()\\\n",
    "    and x[1] <= float_cova:\n",
    "        df_A.pop(x[0])\n",
    "        proteins.append(x[0])\n",
    "\n",
    "int_total_old = len(df_floats.keys())\n",
    "int_discarded = len(proteins)\n",
    "int_total_new = int_total_old - int_discarded\n",
    "\n",
    "print(int_total_old, 'total proteins')\n",
    "print(int_discarded, 'proteins cova <=', float_cova, '(DISCARDED)')\n",
    "print(int_total_new, 'proteins cova >=', float_cova, '(OK)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro passo recomend√°vel √© realizar testes de normalidade nas vari√°veis. Vari√°veis que se desviam muito da normalidade s√£o mais prov√°veis de serem informativas. O teste de Anderson-Darling ou o teste de Shapiro-Wilks funciona bem para esse prop√≥sito.\n",
    "\n",
    "Realize ambos os testes abaixo, plote um histograma com os resultados dos testes para as 77 vari√°veis (prote√≠nas), e verifique:\n",
    "1. Se os dois testes diferem significativamente em seus resultados.\n",
    "2. Se os testes permitem estimar um valor de corte abaixo do qual podemos desprezar as vari√°veis como ‚Äúprovavelmente n√£o-informativas‚Äù.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # anderson-darling\n",
    "# The Anderson-Darling tests the null hypothesis that a sample is drawn from a population\n",
    "# that follows a particular distribution. If the returned statistic is larger than these\n",
    "# critical values then for the corresponding significance level, the null hypothesis that\n",
    "# the data come from the chosen distribution can be rejected.\n",
    "\n",
    "from scipy.stats import anderson\n",
    "from collections import defaultdict\n",
    "\n",
    "# test if possible to reject H0\n",
    "dict_anderson = defaultdict(dict)\n",
    "for x in df.loc[:, df.dtypes == float].keys().tolist():\n",
    "    anderson_stats = anderson(df[x])\n",
    "    anderson_statistic = anderson_stats.statistic\n",
    "    dict_anderson['statistic'][x] = anderson_statistic\n",
    "#     print('\\n'+x, 'Statistic: %.3f' % anderson_statistic)\n",
    "#     for i in range(len(anderson_stats.critical_values)):\n",
    "#         cv = anderson_stats.critical_values[i]\n",
    "#         sl = anderson_stats.significance_level[i]\n",
    "#         if anderson_statistic < cv:\n",
    "#             print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
    "#         else: print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))\n",
    "\n",
    "# save lists of critical values and significance levels\n",
    "anderson_cv = anderson_stats.critical_values.tolist()\n",
    "anderson_sl = anderson_stats.significance_level.tolist()\n",
    "        \n",
    "# transform to series and append to described df\n",
    "series_anderson = pd.Series(dict_anderson['statistic'])\n",
    "resumo.loc['anderson'] = series_anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shapiro-wilks\n",
    "# Esta fun√ß√£o retorna dois valores, o primeiro √© a Estat√≠stica F da amostra\n",
    "# e o segundo √© o valor ùëù da hip√≥tese de que a amostra seja normalmente distribu√≠da.\n",
    "# Assim, se o segundo valor retornado pela fun√ß√£o shapiro for superior ao valor de alfa\n",
    "# isso indica que a amostra deve ser, de fato, normalmente distribu√≠da.\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# get p value for hypothesis\n",
    "dict_shapiro = defaultdict(dict)\n",
    "for x in df.loc[:, df.dtypes == float].keys().tolist():\n",
    "    f, p = shapiro(df[x])\n",
    "    dict_shapiro['f'][x] = f\n",
    "    dict_shapiro['p'][x] = p\n",
    "\n",
    "# transform to series and append to described df\n",
    "series_shapiro = pd.Series(dict_shapiro['p'])\n",
    "resumo.loc['shapiro_p'] = series_shapiro\n",
    "series_shapiro.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapiro histogram\n",
    "plt.hist(series_shapiro, bins='auto', alpha=0.7, rwidth=0.9) # <-- plotar aqui pareceu levar uma eternidade!\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('shapiro p value')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.title('Histograma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anderson histogram\n",
    "plt.hist(series_anderson, bins='auto', alpha=0.7, rwidth=0.9) # <-- plotar aqui pareceu levar uma eternidade!\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('anderson statistic')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.title('Histograma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se for poss√≠vel estabelecer um valor de corte para as vari√°veis, gere um novo dataframe apenas com as prote√≠nas que interessam segundo este crit√©rio (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B = df.copy()\n",
    "proteins = []\n",
    "\n",
    "# considering shapiro p > 0.05\n",
    "for x in series_shapiro.items():\n",
    "    if x[1] > 0.05:\n",
    "        df_B.pop(x[0])\n",
    "        proteins.append(x[0])\n",
    "\n",
    "int_total_old = len(df_floats.keys())\n",
    "int_discarded = len(proteins)\n",
    "int_total_new = int_total_old - int_discarded\n",
    "\n",
    "print(int_total_old, 'total proteins')\n",
    "print(int_discarded, 'proteins look normal (fail to reject H0)')\n",
    "print(int_total_new, 'proteins do not look normal (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe sobreposi√ß√£o entre vari√°veis com alto coeficiente de varia√ß√£o e vari√°veis altamente n√£o-normais? Caso exista, qual √© esta sobreposi√ß√£o?\n",
    "\n",
    "Se criarmos conjuntos a partir dos dataframes gerados pela aplica√ß√£o dos crit√©rio (A) e (B) acima, poderemos ver quais prote√≠nas est√£o presentes em ambos os conjuntos. Fa√ßa isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C = df.copy()\n",
    "\n",
    "proteins = df_floats.keys().tolist()\n",
    "proteins_A = df_A.keys().tolist()\n",
    "proteins_B = df_B.keys().tolist()\n",
    "\n",
    "for x in proteins:\n",
    "    if not all(x in k for k in [proteins_A, proteins_B]):\n",
    "        df_C.pop(x)\n",
    "\n",
    "proteins_C = df_C.loc[:, df.dtypes == float].keys().tolist()\n",
    "print('Proteins =>', proteins_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provavelmente, voc√™ chegou a um n√∫mero de prote√≠nas bem menor do que 77, correto? Essas s√£o as prote√≠nas que tem melhores chances de indicar resultados no tratamento. O que fizemos aqui se chama _redu√ß√£o de dimensionalidade_.\n",
    "\n",
    "Isso nos deixa com um conjunto menor de vari√°veis que foram selecionadas atrav√©s de m√∫ltiplos mecanismos. Vamos em frente e visualiz√°-los todos ao mesmo tempo. Gere, em um √∫nico gr√°fico, o histograma para as concentra√ß√µes de todas as prote√≠nas candidatas que foram selecionadas at√© aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot histogram\n",
    "plt.hist(df_C.mean(), bins='auto', alpha=0.7, rwidth=0.9)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Concentra√ß√£o m√©dia')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.title('Histograma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras m√©tricas interessantes a serem consideradas incluem o _skew_ (inclina√ß√£o) e _kurtosis_ (curtose) das distribui√ß√µes. A inclina√ß√£o mede a simetria de uma distribui√ß√£o sobre sua m√©dia, enquanto a curtose mede a parte dos dados nas caudas da distribui√ß√£o.\n",
    "\n",
    "Skew e kurtosis podem ser combinadas em um bom gr√°fico de dispers√£o que informa muito sobre a estrutura do seu conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defina abaixo o dataframe contendo apenas os atributos num√©ricos (concentra√ß√£o de prote√≠nas)\n",
    "### Sinta-se √† vontade para melhorar o gr√°fico se quiser.\n",
    "\n",
    "df_numeric = df_floats # df.loc[:, df.dtypes == float]\n",
    "\n",
    "skews = df_numeric.apply(lambda x: pd.DataFrame.skew(x))\n",
    "skews.name = \"skew\"\n",
    "\n",
    "kurts = df_numeric.apply(lambda x: pd.DataFrame.kurtosis(x))\n",
    "kurts.name = \"kurtosis\"\n",
    "\n",
    "proteins = pd.Series([i.split(\"_\")[0] for i in kurts.index], index=kurts.index, name=\"protein\")\n",
    "sk_df = pd.concat([skews, kurts, proteins], axis=1)\n",
    "plt.scatter(sk_df['skew'], sk_df['kurtosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No _scatter plot_ acima √© poss√≠vel identificar com certa facilidade os outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tr√™s outliers identificados fora da curva exibida:\n",
    "1. x=~2.5, y=~35\n",
    "2. x=~2.7, y=~43\n",
    "3. x=~4.8, y=~62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write proteins to output file\n",
    "with open('candidate-columns.csv', 'w') as f:\n",
    "    f.write('\\n'.join('%s' % protein for protein in proteins_C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
