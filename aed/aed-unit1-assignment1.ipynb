{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados em Python\n",
    "\n",
    "Neste notebook, vamos praticar algumas das técnicas e processos de pensamento para análise exploratória de dados. Vamos praticar a análise com um conjunto de dados real. É importante enfatizar que não existe um único caminho correto para a análise exploratória de dados. A maneira como você explora seus dados depende das perguntas que você está tentando responder e dos dados em si. Dessa forma, deixe os dados e sua curiosidade guiá-lo.\n",
    "\n",
    "Se for necessário acrescentar passos adicionais além daqueles que estão indicados neste notebook, faça isso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Conjunto de Dados\n",
    "\n",
    "Este conjunto de dados é basicamente uma tabela de amostras com dados de abundância de proteínas nucleares. Essas amostras são de camundongos normais ou de camundongos com uma condição semelhante à _Síndrome de Down_. Alguns desses camundongos receberam uma droga chamada _memantina_, que os pesquisadores conjecturam que poderia resgatar a função cognitiva em camundongos afetados. Existem 38 ratos normais e 34 ratos afetados, e cada rato contribuiu com 15 amostras para o conjunto de dados, para um total de 1080 amostras. A razão de abundância de 77 proteínas é medida para cada amostra.\n",
    "\n",
    "O conjunto de dados pode ser obtido no [aqui](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression). Acesse o link `Data Folder`, e salve o arquivo `Data_Cortex_Nuclear.xls` na mesma pasta em que você gravou este notebook. Baixe este arquivo, e carregue-o em um dataframe do Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data_Cortex_Nuclear.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-75f015b56dc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data_Cortex_Nuclear.xls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, parse_cols, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skip_footer, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     return io.parse(\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_stringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m~/scripts/chn@git/chn_analysis_tools/chn/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_Cortex_Nuclear.xls'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('Data_Cortex_Nuclear.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em seguida inspecione o dataframe para verificar se há dados faltantes. Caso haja, preencha estes dados com a média dos dados existentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_df_nulls(df, verbose=False):\n",
    "    \n",
    "    # get columns with float64 values\n",
    "    is_float = []\n",
    "    for i in df.dtypes.keys():\n",
    "        if df.dtypes[i] == 'float64':\n",
    "            is_float.append(i)\n",
    "\n",
    "    # get columns with null values      \n",
    "    has_null = df.isnull().any()\n",
    "    for i in has_null.keys():\n",
    "        if has_null[i]:\n",
    "            print(i, '=>', df[i].isnull().sum(), 'nulls')\\\n",
    "            if verbose else None\n",
    "        else: has_null.pop(i)\n",
    "\n",
    "    # get columns with null float64 values\n",
    "    to_fill = [value for value in is_float if value in has_null.keys()]\n",
    "\n",
    "    # print overview\n",
    "    print(len(df.keys()), 'total columns')\n",
    "    print(len(is_float), 'are float64')\n",
    "    print(len(has_null), 'contain nulls')\n",
    "    print(len(to_fill), 'columns to fill\\n')\n",
    "    # return to_fill\n",
    "\n",
    "# first check\n",
    "print('Checking data frame...')\n",
    "check_df_nulls(df)\n",
    "    \n",
    "# fill values with mean\n",
    "print('Filling null values with mean...', end=' ')\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# for i in to_fill:\n",
    "#     df[i] = df[i].fillna(df[i].mean())\n",
    "    \n",
    "# second check\n",
    "print('OK.\\n\\nRechecking...')\n",
    "check_df_nulls(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Estatísticas de Resumo Adicionais\n",
    "\n",
    "Os dataframes do Pandas têm um método prático chamado `describe` que escreve uma variedade de estatísticas de resumo para cada variável. As estatísticas de resumo retornadas por esse método também são um dataframe, portanto, você pode alterá-lo facilmente para incluir outras estatísticas. Vamos fazer exatamente isso, adicionando o coeficiente de variação da variável (cova).\n",
    "\n",
    "O coeficiente de variação é apenas a razão entre o desvio padrão e a média da distribuição. Distribuições com um grande coeficiente de variação são mais propensas a sinalizar uma mudança no processo de geração de dados subjacente, ao contrário da variação aleatória.\n",
    "\n",
    "Adicionar uma nova linha no dataframe de `resumo` pode ser realizado da seguinte forma `resumo.loc['cova'] = «dados da nova linha»`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resumo = df.describe()\n",
    "resumo.loc['cova'] = df.std()/df.mean()\n",
    "resumo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização\n",
    "\n",
    "Um passo inicial importante antes de qualquer análise real é realizado para normalizar os dados. Isso é necessário para evitar que variáveis com valores grandes tenham uma influência indevida nos resultados. Existem alguns métodos diferentes de normalizar seus dados:\n",
    "\n",
    "+ O método mais comum é dividir os valores de uma dimensão pelo maior valor que ela contém. Esse método é fácil de interpretar e é robusto para outliers quando o coeficiente de variação em uma dimensão é baixo.\n",
    "+ Outro método muito comum é subtrair a média e dividir a dimensão por seu desvio padrão, um processo às vezes chamado de padronização. Esse método fornece mais informações sobre a similaridade relativa de valores quando o coeficiente de variação em uma dimensão é alto. Desempenha mal nas dimensões com um baixo coeficiente de variação e grandes outliers.\n",
    "+ Uma opção subutilizada é usar a função de distribuição cumulativa da distribuição empírica da dimensão para obter uma classificação de percentil. Esse método funciona bem na maioria dos casos e é robusto para outliers. Sua utilidade se degrada um pouco quando o coeficiente de variação de uma dimensão fica muito pequeno.\n",
    "\n",
    "O método de normalização usado depende dos dados e dos aspectos que você deseja destacar. Você pode usar um método de normalização diferente para cada dimensão de seus dados, embora eu não recomende isso, a menos que seja absolutamente necessário. Em geral, minha preferência é pelo método de distribuição empírica.\n",
    "\n",
    "O código em Python necessário para fazer os 3 tipos de normalização é exemplificado abaixo (supondo que o dataframe original se chame `df` e contenha apenas dados numéricos):\n",
    "\n",
    "    minmax_normalized_df = pd.DataFrame(MinMaxScaler().fit_transform(df), \n",
    "    columns=df_numeric.columns, index=df_numeric.index)\n",
    "    \n",
    "    standardized_df = pd.DataFrame(StandardScaler().fit_transform(df), \n",
    "    columns=df_numeric.columns, index=df_numeric.index)\n",
    "    \n",
    "    ecdf_normalized_df = df.apply(lambda c: pd.Series(ECDF(c)(c), index=c.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns with float values from data frame\n",
    "df_floats = df.loc[:, df.dtypes == float]\n",
    "\n",
    "# normalize values in data frame (MinMax)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "normalized_df = pd.DataFrame(MinMaxScaler().fit_transform(df_floats),\n",
    "                             columns=df_floats.columns, index=df.index)\n",
    "\n",
    "# normalize values in data frame (Standard)\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# normalized_df = pd.DataFrame(StandardScaler().fit_transform(df_floats),\n",
    "#                              columns=df_floats.columns, index=df.index)\n",
    "\n",
    "# normalize data frame (ECDF)\n",
    "# from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# normalized_df = df_floats.apply(lambda c: pd.Series(ECDF(c)(c), index=c.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando dados\n",
    "\n",
    "Usaremos a estratégia de explorar dados procurando coisas que são fora do comum. Em um conjunto de dados que seja apenas variáveis aleatórias gaussianas não correlacionadas, não poderiamos aprender nada com ele, seria literalmente ruído branco. A maior parte do conteúdo da informação na maioria dos conjuntos de dados é sua variação da normalidade.\n",
    "\n",
    "A distribuição do coeficiente de variação informa sobre a variabilidade relativa desses dados. Altos coeficientes de variação para uma variável indicam um provável candidato a efeito experimental.\n",
    "\n",
    "Plote o histograma do coeficiente de variação dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# get coefficient_of_variation\n",
    "series_cova = normalized_df.std()/normalized_df.mean()\n",
    "series_cova.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram\n",
    "plt.hist(series_cova, bins=30, alpha=0.7, rwidth=0.9)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Coeficiente de variação')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Histograma')\n",
    "# plt.xlim(left=0.15, right=0.55) # <-- slice it further\n",
    "plt.show()\n",
    "\n",
    "# plot more histograms\n",
    "# for i in df.keys():\n",
    "#     if df.dtypes[i] == 'float64':\n",
    "#         plt.hist(normalized_df[i], bins='auto', alpha=0.7, rwidth=0.9)\n",
    "#         plt.grid(axis='y', alpha=0.75)\n",
    "#         plt.xlabel(str(i))\n",
    "#         plt.ylabel('Frequency')\n",
    "#         plt.title(i)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do gráfico acima é possível estimar um valor de corte para o coeficiente de variação? Um valor abaixo do qual poderíamos desprezar a alteração das proteínas como sendo um efeito do tratamento? Qual seria este valor?\n",
    "\n",
    "Se você achar que sim, gere um novo dataframe apenas com as proteínas que interessam segundo este critério (A).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta**: Considerando o gráfico acima, argumenta-se que o valor de corte deve visar as proteínas que obtiveram um coeficiente de variação de aproximadamente >= 0.496 entre si, por razão:\n",
    "- a) da grande quantidade de proteínas que exibiram esse padrão (de pouco variar entre si);\n",
    "- b) de que distribuições com um grande coeficiente de variação são mais propensas a sinalizar uma mudança no processo de geração de dados subjacente, ao contrário da variação aleatória.\n",
    "\n",
    "Nota-se que uma análise exploratória de cada variável em mais histogramas no conjunto subsequente pode ser útil para o entendimento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A = df.copy()\n",
    "proteins = []\n",
    "\n",
    "# considering <= 0.4\n",
    "float_cova = 0.4\n",
    "for x in series_cova.items():\n",
    "    if x[0] in df_A.keys()\\\n",
    "    and x[1] <= float_cova:\n",
    "        df_A.pop(x[0])\n",
    "        proteins.append(x[0])\n",
    "\n",
    "int_total_old = len(df_floats.keys())\n",
    "int_discarded = len(proteins)\n",
    "int_total_new = int_total_old - int_discarded\n",
    "\n",
    "print(int_total_old, 'total proteins')\n",
    "print(int_discarded, 'proteins cova <=', float_cova, '(DISCARDED)')\n",
    "print(int_total_new, 'proteins cova >=', float_cova, '(OK)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outro passo recomendável é realizar testes de normalidade nas variáveis. Variáveis que se desviam muito da normalidade são mais prováveis de serem informativas. O teste de Anderson-Darling ou o teste de Shapiro-Wilks funciona bem para esse propósito.\n",
    "\n",
    "Realize ambos os testes abaixo, plote um histograma com os resultados dos testes para as 77 variáveis (proteínas), e verifique:\n",
    "1. Se os dois testes diferem significativamente em seus resultados.\n",
    "2. Se os testes permitem estimar um valor de corte abaixo do qual podemos desprezar as variáveis como “provavelmente não-informativas”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # anderson-darling\n",
    "# The Anderson-Darling tests the null hypothesis that a sample is drawn from a population\n",
    "# that follows a particular distribution. If the returned statistic is larger than these\n",
    "# critical values then for the corresponding significance level, the null hypothesis that\n",
    "# the data come from the chosen distribution can be rejected.\n",
    "\n",
    "from scipy.stats import anderson\n",
    "from collections import defaultdict\n",
    "\n",
    "# test if possible to reject H0\n",
    "dict_anderson = defaultdict(dict)\n",
    "for x in df.loc[:, df.dtypes == float].keys().tolist():\n",
    "    anderson_stats = anderson(df[x])\n",
    "    anderson_statistic = anderson_stats.statistic\n",
    "    dict_anderson['statistic'][x] = anderson_statistic\n",
    "#     print('\\n'+x, 'Statistic: %.3f' % anderson_statistic)\n",
    "#     for i in range(len(anderson_stats.critical_values)):\n",
    "#         cv = anderson_stats.critical_values[i]\n",
    "#         sl = anderson_stats.significance_level[i]\n",
    "#         if anderson_statistic < cv:\n",
    "#             print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
    "#         else: print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))\n",
    "\n",
    "# save lists of critical values and significance levels\n",
    "anderson_cv = anderson_stats.critical_values.tolist()\n",
    "anderson_sl = anderson_stats.significance_level.tolist()\n",
    "        \n",
    "# transform to series and append to described df\n",
    "series_anderson = pd.Series(dict_anderson['statistic'])\n",
    "resumo.loc['anderson'] = series_anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shapiro-wilks\n",
    "# Esta função retorna dois valores, o primeiro é a Estatística F da amostra\n",
    "# e o segundo é o valor 𝑝 da hipótese de que a amostra seja normalmente distribuída.\n",
    "# Assim, se o segundo valor retornado pela função shapiro for superior ao valor de alfa\n",
    "# isso indica que a amostra deve ser, de fato, normalmente distribuída.\n",
    "\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# get p value for hypothesis\n",
    "dict_shapiro = defaultdict(dict)\n",
    "for x in df.loc[:, df.dtypes == float].keys().tolist():\n",
    "    f, p = shapiro(df[x])\n",
    "    dict_shapiro['f'][x] = f\n",
    "    dict_shapiro['p'][x] = p\n",
    "\n",
    "# transform to series and append to described df\n",
    "series_shapiro = pd.Series(dict_shapiro['p'])\n",
    "resumo.loc['shapiro_p'] = series_shapiro\n",
    "series_shapiro.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shapiro histogram\n",
    "plt.hist(series_shapiro, bins='auto', alpha=0.7, rwidth=0.9) # <-- plotar aqui pareceu levar uma eternidade!\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('shapiro p value')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Histograma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anderson histogram\n",
    "plt.hist(series_anderson, bins='auto', alpha=0.7, rwidth=0.9) # <-- plotar aqui pareceu levar uma eternidade!\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('anderson statistic')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Histograma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se for possível estabelecer um valor de corte para as variáveis, gere um novo dataframe apenas com as proteínas que interessam segundo este critério (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_B = df.copy()\n",
    "proteins = []\n",
    "\n",
    "# considering shapiro p > 0.05\n",
    "for x in series_shapiro.items():\n",
    "    if x[1] > 0.05:\n",
    "        df_B.pop(x[0])\n",
    "        proteins.append(x[0])\n",
    "\n",
    "int_total_old = len(df_floats.keys())\n",
    "int_discarded = len(proteins)\n",
    "int_total_new = int_total_old - int_discarded\n",
    "\n",
    "print(int_total_old, 'total proteins')\n",
    "print(int_discarded, 'proteins look normal (fail to reject H0)')\n",
    "print(int_total_new, 'proteins do not look normal (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existe sobreposição entre variáveis com alto coeficiente de variação e variáveis altamente não-normais? Caso exista, qual é esta sobreposição?\n",
    "\n",
    "Se criarmos conjuntos a partir dos dataframes gerados pela aplicação dos critério (A) e (B) acima, poderemos ver quais proteínas estão presentes em ambos os conjuntos. Faça isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_C = df.copy()\n",
    "\n",
    "proteins = df_floats.keys().tolist()\n",
    "proteins_A = df_A.keys().tolist()\n",
    "proteins_B = df_B.keys().tolist()\n",
    "\n",
    "for x in proteins:\n",
    "    if not all(x in k for k in [proteins_A, proteins_B]):\n",
    "        df_C.pop(x)\n",
    "\n",
    "proteins_C = df_C.loc[:, df.dtypes == float].keys().tolist()\n",
    "print('Proteins =>', proteins_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provavelmente, você chegou a um número de proteínas bem menor do que 77, correto? Essas são as proteínas que tem melhores chances de indicar resultados no tratamento. O que fizemos aqui se chama _redução de dimensionalidade_.\n",
    "\n",
    "Isso nos deixa com um conjunto menor de variáveis que foram selecionadas através de múltiplos mecanismos. Vamos em frente e visualizá-los todos ao mesmo tempo. Gere, em um único gráfico, o histograma para as concentrações de todas as proteínas candidatas que foram selecionadas até aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# plot histogram\n",
    "plt.hist(df_C.mean(), bins='auto', alpha=0.7, rwidth=0.9)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Concentração média')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Histograma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outras métricas interessantes a serem consideradas incluem o _skew_ (inclinação) e _kurtosis_ (curtose) das distribuições. A inclinação mede a simetria de uma distribuição sobre sua média, enquanto a curtose mede a parte dos dados nas caudas da distribuição.\n",
    "\n",
    "Skew e kurtosis podem ser combinadas em um bom gráfico de dispersão que informa muito sobre a estrutura do seu conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defina abaixo o dataframe contendo apenas os atributos numéricos (concentração de proteínas)\n",
    "### Sinta-se à vontade para melhorar o gráfico se quiser.\n",
    "\n",
    "df_numeric = df_floats # df.loc[:, df.dtypes == float]\n",
    "\n",
    "skews = df_numeric.apply(lambda x: pd.DataFrame.skew(x))\n",
    "skews.name = \"skew\"\n",
    "\n",
    "kurts = df_numeric.apply(lambda x: pd.DataFrame.kurtosis(x))\n",
    "kurts.name = \"kurtosis\"\n",
    "\n",
    "proteins = pd.Series([i.split(\"_\")[0] for i in kurts.index], index=kurts.index, name=\"protein\")\n",
    "sk_df = pd.concat([skews, kurts, proteins], axis=1)\n",
    "plt.scatter(sk_df['skew'], sk_df['kurtosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No _scatter plot_ acima é possível identificar com certa facilidade os outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Três outliers identificados fora da curva exibida:\n",
    "1. x=~2.5, y=~35\n",
    "2. x=~2.7, y=~43\n",
    "3. x=~4.8, y=~62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write proteins to output file\n",
    "with open('candidate-columns.csv', 'w') as f:\n",
    "    f.write('\\n'.join('%s' % protein for protein in proteins_C))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
